This chapter shows the experiment results of the reproduced baseline and our approach of {\simfinmo}.
Table \ref{tab:precision}- \ref{tab:mcc} shows the overall results of the baseline and our approach.
The bold values in the table indicates the highest score from the projects.


\section{RQ1: {\simfinmo} vs Baseline}
From Table \ref{tab:precision}, we can see that all most all of the baseline results are better in precision.
However, Table \ref{tab:recall} shows that {\simfinmo} always has better performance in recall.
Due to precision and recall having trade-offs with each other, it is better to see the F-measure which is the harmonic mean of precision and recall.
From Table \ref{tab:f1score}, we can see that out of 6 projects, {\simfinmo} outperforms 4 of the projects in F1 score. 
The average F1 score of {\simfinmo} is also the highest out of all the baseline machine learning algorithms.
Table \ref{tab:mcc} shows a mixed result, but overall, random forest shows the best performance in terms of MCC with the highest average.
From the results, we can state that {\simfinmo} outperforms the baseline overall.

% Precision
\begin{table}[!ht]
\caption{Precision value of each baseline and \simfinmo}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Name & BN & IBk & J48 & LMT & NB & RF & SFM \\ \hline
maven & 0.220 & 0.141 & 0.253 & 0.297 & 0.147 & \textbf{0.362} & 0.146 \\ \hline
ranger & 0.200 & 0.144 & 0.162 & 0.141 & 0.182 & \textbf{0.350} & 0.183 \\ \hline
sentry & 0.154 & 0.065 & 0.168 & 0.160 & 0.098 & \textbf{0.233} & 0.141 \\ \hline
sqoop & 0.220 & 0.153 & 0.205 & 0.243 & 0.170 & \textbf{0.379} & 0.036 \\ \hline
syncope & 0.268 & 0.099 & 0.209 & 0.200 & 0.076 & \textbf{0.280} & 0.056 \\ \hline
tez & 0.161 & 0.173 & 0.232 & 0.159 & 0.150 & \textbf{0.265} & 0.241 \\ \hline
average & 0.204 & 0.129 & 0.205 & 0.200 & 0.137 & \textbf{0.311} & 0.134 \\ \hline
\end{tabular}%

\label{tab:precision}
\end{table}

% Recall
\begin{table}[!ht]
\caption{Recall value of each baseline and \simfinmo.}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Name & BN & IBk & J48 & LMT & NB & RF & SFM \\ \hline
maven & 0.114 & 0.190 & 0.126 & 0.103 & 0.322 & 0.065 & \textbf{0.834} \\ \hline
ranger & 0.539 & 0.162 & 0.267 & 0.278 & 0.831 & 0.125 & \textbf{0.886} \\ \hline
sentry & 0.294 & 0.120 & 0.160 & 0.184 & 0.341 & 0.104 & \textbf{0.887} \\ \hline
sqoop & 0.207 & 0.193 & 0.213 & 0.261 & 0.441 & 0.107 & \textbf{0.846} \\ \hline
syncope & 0.023 & 0.078 & 0.040 & 0.009 & 0.172 & 0.019 & \textbf{0.877} \\ \hline
tez & 0.298 & 0.355 & 0.371 & 0.528 & 0.665 & 0.302 & \textbf{0.841} \\ \hline
average & 0.246 & 0.183 & 0.196 & 0.227 & 0.462 & 0.120 & \textbf{0.862} \\ \hline
\end{tabular}%
\label{tab:recall}
\end{table}


% F-1 Score
\begin{table}[!ht]
\caption{F1 score of each basline and \simfinmo}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Name & BN & IBk & J48 & LMT & NB & RF & SFM \\ \hline
maven & 0.150 & 0.162 & 0.168 & 0.153 & 0.202 & 0.111 & \textbf{0.248} \\ \hline
ranger & 0.292 & 0.152 & 0.202 & 0.187 & 0.299 & 0.184 & \textbf{0.303} \\ \hline
sentry & 0.202 & 0.085 & 0.164 & 0.171 & 0.153 & 0.143 & \textbf{0.243} \\ \hline
sqoop & 0.213 & 0.171 & 0.209 & \textbf{0.251} & 0.245 & 0.167 & 0.068 \\ \hline
syncope & 0.042 & 0.087 & 0.068 & 0.017 & \textbf{0.106} & 0.036 & 0.105 \\ \hline
tez & 0.209 & 0.232 & 0.285 & 0.244 & 0.244 & 0.282 & \textbf{0.375} \\ \hline
average & 0.185 & 0.148 & 0.183 & 0.170 & 0.208 & 0.154 & \textbf{0.224} \\ \hline
\end{tabular}%
\label{tab:f1score}
\end{table}


% MCC
\begin{table}[!ht]
\caption{MCC of each baseline and \simfinmo}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Name& BN & IBk & J48 & LMT & NB & RF & SFM \\ \hline
maven & 0.096 & 0.058 & 0.119 & \textbf{0.126} & 0.089 & 0.120 & 0.056 \\ \hline
ranger & 0.150 & 0.015 & 0.047 & 0.017 & \textbf{0.182} & 0.143 & 0.095 \\ \hline
sentry & 0.098 & -0.043 & 0.080 & 0.079 & 0.013 & 0.100 & \textbf{0.101} \\ \hline
sqoop & 0.084 & 0.011 & 0.071 & 0.119 & 0.053 & \textbf{0.137} & 0.047 \\ \hline
syncope & 0.059 & 0.021 & \textbf{0.061} & 0.027 & -0.001 & 0.056 & 0.051 \\ \hline
tez & 0.051 & 0.076 & 0.155 & 0.074 & 0.068 & \textbf{0.167} & 0.033 \\ \hline
average & 0.089 & 0.023 & 0.089 & 0.074 & 0.067 & \textbf{0.121} & 0.064 \\ \hline
\end{tabular}%
\label{tab:mcc}
\end{table}


\section{RQ2: How actionable is {\simfinmo}?}
To show how much the patches that are suggested by {\simfinmo} are helpful to developers, we manually inspected the patches and labeled each of them.
We only inspected patches that were predicted to be a true positive.
We also only considered patches were the ground truth patches are short. 
The reason we only considered short patches because most of the longer patches composed refactoring and architecture changes.
With these kind of patches, it is difficult analyze the patches correctly, due to the variety of number and domain of the used projects.
The labels our are as follows:
\begin{enumerate}
    \item \textbf{Direct hint}: the ground truth patch and the suggested patch are same changes respect to their AST node type.
    \item \textbf{Indirect hint}: the ground truth patch and the suggested patch are not the same changes respect to their AST node type, however, they have similar node types with high relevance.
    \item \textbf{No hint}: the ground truth patch and the suggested patch are does not have the same changes respect to their AST node type and has no relevance at all.
\end{enumerate}
The labeling can be very subjective as to say that changes have relevance or not.
So we include some examples to show how the manual labels are done.

In Fig. \ref{fig:direct_hint}, there are three changes: the target BIC change, ground truth patch of the target BIC change, and the suggested patch. 
By looking at the the ground truth patch, we can see that the intialization of a string variable contatains bug as it is replaced with another value (true -> false).
While they have different values, we can see that the suggested patch also changes a public static final String value.
In cases where the changed AST node types are the same between ground truth patch and the suggested patch, we label them as \emph{Direct hint}.

Similar example is shown in Fig. \ref{fig:indirect_hint}.
The introduced bug has to do with ommitting a parameter in a method implementation.
The suggeseted patch also shows that it added a method paramenter.
However, it is happening in a method invocation line of code.
And the type of the changed parameter is most likely to be a different type.
Eventhough the change contains different types of AST node, we can still infer that omission of a parameter can help fix the bug.
In these kind of cases, we label them as \emph{Indirect hint}.

Through this method of labeling, we inspected all the test projects.
Table \ref{tab:rq_2} shows the statistics of the suggested patch labeling.
We can see that 28\% ~ 60\% of the patches are shows direct or indirect hint.
The percentage of the hint respect to the suggested patch can be shown as low.
However, it is encouraging tho see this result because no other method provided an actionable message this direct in other defect prediction methods.
And to consider that the result only shows the top 1 suggestions, we can hope that the hint rate will be higher when we consider more top k instances.

\begin{figure*}[!tbp]
\renewcommand{\arraystretch}{1}
    \centering
    \includegraphics[width=\textwidth]{figures/direct_hint.png}\hfill
    \caption{An example of a direct hint in labeling suggested patches.}
    \label{fig:direct_hint}
\end{figure*}

\begin{figure*}[!tbp]
\renewcommand{\arraystretch}{1}
    \centering
    \includegraphics[width=\textwidth]{figures/indirect_hint.png}\hfill
    \caption{An example of a indirect hint in labeling suggested patches.}
    \label{fig:indirect_hint}
\end{figure*}

% RQ2
\begin{table}[!htp]
\centering
\caption{A table that shows the statistics of suggested patches and their labeling.}
\begin{tabular}{|l|l|l|l|}
\hline
{Name} & {Direct hint} & {Indirect hint} & {No hint} \\ \hline
{maven} & 6 (18\%) & 8 (23\%) & 20 (59\%) \\ \hline
{ranger} & 11 (16\%) & 16 (24\%) & 40 (60\%) \\ \hline
{sentry} & 4 (14\%) & 7 (24\%) & 18 (62\%) \\ \hline
{sqoop} & 3 (30\%) & 3 (30\%) & 4 (40\%) \\ \hline
{syncope} & 9 (12\%) & 14 (19\%) & 52 (69\%) \\ \hline
{tez} & 10 (14\%) & 10 (14\%) & 70 (72\%) \\ \hline
\end{tabular}%
\label{tab:rq_2}
\end{table}


\section{RQ3: {\simfinmo} with different cut-off values}
To provide a better concept of how well the model {\simfinmo} predicts defective modules, we have investigated the different performance values with different cut-off values.
The results are tabulated in Table \ref{tab:rq_3}.
Due to the limitation of space in the report, we have only tabulated one of the test projects, maven.
From the table, we can see that precision score is highest when the cut-off value is low (closer to zero).
However, the recall value is the lowest.
The precision score peaks when the cut-off ranges from 0.000001 to 0.1 for other projects as well.
On the other hand, precision drops pretty low when the cut-of value goes over 1.
However, recall starts to go up rapidly as the cut-off value gets higher.
The ascending of the recall is much higher than the descending of the precision yielding a good value of f1-score. 

% RQ3.
\begin{table}[!ht]
\centering
\caption{This table shows different performance metrics using different cut-off values.
The following result is from the project maven. }
\begin{tabular}{|c|c|c|c|c|}
\hline
Cut-off & \multicolumn{1}{c|}{Precision} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{F1 Score} & \multicolumn{1}{c|}{MCC} \\ \hline
0.1 & \textbf{0.286} & 0.002 & 0.004 & 0.014 \\ \hline
0.2 & 0.167 & 0.003 & 0.006 & 0.005 \\ \hline
0.3 & 0.183 & 0.036 & 0.061 & 0.023 \\ \hline
0.4 & 0.190 & 0.146 & 0.165 & 0.054 \\ \hline
0.5 & 0.196 & 0.351 & 0.252 & 0.100 \\ \hline
0.6 & 0.194 & 0.531 & \textbf{0.284} & \textbf{0.131} \\ \hline
0.7 & 0.177 & 0.650 & 0.278 & 0.120 \\ \hline
0.8 & 0.155 & 0.727 & 0.255 & 0.073 \\ \hline
0.9 & 0.149 & 0.797 & 0.250 & 0.062 \\ \hline
1 & 0.146 & 0.837 & 0.248 & 0.056 \\ \hline
2 & 0.144 & 0.924 & 0.249 & 0.063 \\ \hline
3 & 0.144 & 0.934 & 0.250 & 0.066 \\ \hline
4 & 0.144 & 0.936 & 0.250 & 0.066 \\ \hline
5 & 0.144 & 0.939 & 0.250 & 0.068 \\ \hline
6 & 0.144 & 0.940 & 0.250 & 0.067 \\ \hline
7 & 0.143 & 0.940 & 0.249 & 0.064 \\ \hline
8 & 0.143 & 0.940 & 0.248 & 0.062 \\ \hline
9 & 0.143 & 0.942 & 0.249 & 0.064 \\ \hline
10 & 0.143 & \textbf{0.942} & 0.249 & 0.064 \\ \hline
\end{tabular}%
\label{tab:rq_3}
\end{table}


\section{RQ4: Combined {\simfin} vs. Divided {\simfin}}
In section IV, we have talked about how we decided on choosing one combind design of {\simfin} or buggy/clean divided design of {\simfin}.
To show how different design of {\simfin} contribute to the prediction performance of {\simfinmo}, we compare their result.
In table \ref{tab:rq_4}, we can see that the divided {\simfin} outperforms the combined {\simfin} in precision, recall, and f1-score of every test projects.
It also outperforms 4 out of 6 test projects in MCC.
Through this result, we think that dividing the encoding space of buggy and clean changes helps retrieving the representative distance from the target change.

% RQ4.
\begin{table}[!ht]
\centering
\caption{This table shows prediction performance of combined {\simfin} and two divided {\simfin}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|l|l|l|l|l|l|l|l|}
\hline
\multirow{2}{*}{} & \multicolumn{4}{c|}{combined {\simfin}} & \multicolumn{4}{c|}{divided {\simfin}} \\ \cline{2-9} 
Name & Precision & Recall & F1-score & MCC & Precision & Recall & F1-Score & MCC \\ \hline
maven & 0.109 & 0.660 & 0.187 & \textbf{0.093} & \textbf{0.146} & \textbf{0.834} & \textbf{0.248} & 0.056 \\ \hline
ranger & 0.123 & 0.726 & 0.210 & 0.060 & \textbf{0.183} & \textbf{0.886} & \textbf{0.303} & \textbf{0.095} \\ \hline
sentry & 0.115 & 0.709 & 0.198 & 0.071 & \textbf{0.141} & \textbf{0.887} & \textbf{0.243} & \textbf{0.101} \\ \hline
sqoop & 0.029 & 0.495 & 0.055 & 0.042 & \textbf{0.036} & \textbf{0.846} & \textbf{0.068} & \textbf{0.047} \\ \hline
syncope & 0.054 & 0.684 & 0.100 & 0.048 & \textbf{0.056} & \textbf{0.877} & \textbf{0.105} & \textbf{0.051} \\ \hline
tez & 0.161 & 0.570 & 0.252 & \textbf{0.058} & \textbf{0.241} & \textbf{0.841} & \textbf{0.375} & 0.033 \\ \hline
\end{tabular}%
}
\label{tab:rq_4}
\end{table}

% \section{Analysis}
% Table \ref{tab:precision}- \ref{tab:mcc} shows the prediction performances between baseline and {\simfinmo} in various measures such as precision, recall, f1-score, and MCC.

% We use Friedman and Nemenyi test to statistically evaluate the performance of algorithms of {\simfinmo} and baseline. Friedman test is a non-parametric test to determine the statistical significant of the data that is classification algorithm, and usually comparing three or more data. In this paper, Friedman test is used to compare the statistical significance of evaluation metrics of all the classifiers of each project. The outputs of Friedman test are degree of freedom that is the maximum number of logically independent values, Friedman chi-squared that if the value is large, there is a relationship and if it is small, there isn't relationship and p-value that mean the relationship is statistically significant when the value is less than 0.05.
% The p-value of precision is  0.008772, and Friedman chi-squared is 17.143. The p-value of recall is 5.88E-06, and Friedman chi-squared is 34.303. Lastly, p-value of f-measure is 0.2179, and Friedman chi-squared is 8.2857. They all have the same degree of freedom that is 6. As a result, since the p-value of precision and recall is less than 0.05, it was statistically verified that there is a difference in performance of the algorithm. Nemenyi test has characteristics similar to Friedman test since it is usually conducted after Friedman test. It compare statistical significant between two pairwise data. By Friedman test we found that there was a difference in the defect prediction performance of the algorithm. Therefore, we conduct Nemenyi test which calculate two algorithms difference in performance. P-value is created by comparing seven classifiers with other classifiers other than oneself.