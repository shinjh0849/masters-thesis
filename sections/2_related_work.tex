\section{Similar Commit Search}
The key part of our approach is that {\simfin} finds which commits are similar to the target commit.
Existing studies that are related to this technique are code clone detection/search, code search engines, commit clustering.

\subsection{Code Clone Detection/Search}
Jiang et al. \cite{jiang2007deckard} proposed an approach named DECARD which represents code blocks as subtrees and uses similarity algorithms on tree data structures.
Lee et al. \cite{lee2010instant} proposed a method that uses a multi-dimensional indexing technique and kNN (k-Nearest Neighborhood) algorithm to reduce the search time while maintaining the functionality of finding semantically similar code fragments.
They used 54 MLOC of code to make this code clone detection module.
Keivanloo et al. \cite{keivanloo2011internet} did a similar research, but their main difference is that they used hash tables and binary search algorithms in implementing a multi-level indexing technique.
They experimented and evaluated on 266 MLOC of codebases.
White et al. \cite{white2016deep} exploited deep learning techniques that are used in natural language processing (e.g. Recursive Neural Network, or Recurrent Neural Network) to extract syntactical patterns and detect code clones with similar patterns. 

The difference between code clone detection/search and similar commit search is that they have different structures of codebases.
Commit shows how code is changed from one code to the other which contains information such as which nodes are added, deleted, updated, or moved, or the metadata of the commit such as which developer is responsible for the change, time of commit, number of changed files, and so on.
Due to their structural differences, the necessity of studying a different approach is evident. 

\subsection{Code Search Engine}
Bajracharya et al. \cite{bajracharya2006sourcerer} proposed a tool named Sourcerer which searches for code fragments.
The tool divides target code fragments concerning the code used to improve the search rate.
They have divided the categories into implementation, uses, and structures.
McMillan et al. \cite{mcmillan2011exemplar} proposed Exemplar (\underline{Exe}cutable exa\underline{mpl}es \underline{ar}chive) helps find code fragments that function as the natural language query input.
This study focused on improving the search rate by reducing the gap between the high abstraction of natural language query and low-level language of source code.
Kim et al. \cite{kim2010towards} proposed a tool that when a user searches for an API document, returns code snippets that can be helpful for the API's usage together with the document.
Kim et al. \cite{kim2018facoy} proposed FACOY (\underline{F}ind \underline{A} \underline{C}ode \underline{O}ther than \underline{Y}ours) searches for a code fragment that is similar to the user's input but syntactically or semantically but with a similar function.
Gu et al. \cite{gu2018deep} proposed CODEnn (\underline{Co}de-\underline{D}escription \underline{E}mbedding \underline{n}eural \underline{n}etwork) to find a semantical significance of the natural language query and the target code snippet.
They do this by mapping both natural language and code snippet in high dimensional vector space and trains a deep learning model to map these instances as close to space if they have semantically similar.

The difference between code search and commit search is that static code and commit has a different structure, just as code clone search.
Second, code search techniques are more focused on handling natural language queries as input.
While some do handle code fragments, they are limited in their ability to handle longer code fragments. 
To do a fully commit search, we must be able to handle longer codebases as input to search for commits because commits can be very long.

\subsection{Commit Clustering}
Kreutzer et al. \cite{kreutzer2016automatic} did a study about clustering similar commits concerning their major functions (e.g. bug fixing, refactoring, etc.).
To do that, they have extracted commits that are in existing software repositories such as Git and applied the LCS (Longest Common Subsequence) algorithm to retrieve a matrix of commit's similarities.
With this matrix, they applied two kinds of clustering algorithms to categorize commits that have similar scores.
Dias et al. \cite{dias2015untangling} did similar work but with different scope.
They categorized different changes to commit concerning different intentions.
They studied this because, with a single commit, developers change several files that are sometimes nothing to do with their intentions (i.e. tangled change).
To do this, they used IDE activity history, and applied different machine learning algorithms (i.e. binary logistic regression, random forest, na√Øve Bayes, etc.) for classification, and applied hierarchical clustering to cluster them.

The difference between commit clustering and commit search is that in \cite{kreutzer2016automatic}, the clusterings are too big to find the syntactical or semantical similarities of each commit. And as for \cite{dias2015untangling}, the granularity of change is within a single commit, making it hard to scale up to search similar commits in other projects.

\section{Defect Prediction}
In this section, we survey the various SDP methods and explain how they are different from each other and our work.

\subsection{Traditional Defect}
Traditional defect prediction predicts a module in different granularity as buggy or clean.
In a traditional defect prediction scenario, the granularity is usually in the file-level or the method level.
They use the previous version of their project to predict the current or later version of the project.
Munson et al. \cite{munson1992detection} built a classification model to classify if a module has a high risk or not with an accuracy of 92\%.
Chidamber and Kemerer \cite{chidamber1994metrics} proposed a suite of object-oriented related metrics that could be applied in defect prediction.
Nagappan and Ball \cite{nagappan2005use} proposed code churn metrics to predict the defect density of the system.
This was the first process-related metrics and more process-related metrics were proposed after.

\subsection{Cross-Project Defect Prediction}
Cross-project defect prediction (CPDP) was proposed to alleviate the cold-start problem of the traditional defect prediction because traditional defect prediction relied on previous versions of the target project.
For a project with little or no previous data, it is very hard or impossible to apply defect prediction.
So CPDP uses data from other projects to learn the prediction model.
Watanabe et al. \cite{watanabe2008adapting} proposed the first CPDP approach to apply prediction models that are already built for other projects.
Ma et al. \cite{ma2012transfer} proposed Transfer Naive Bayes (TNB) that weights source instances similar to the target instances
Nam et al. \cite{nam2013transfer} proposed TCA+ to alleviate the feature differencing problem in applying CPDP.

\subsection{Heterogeneous Defect Prediction}
Heterogeneous defect prediction was first proposed by \cite{nam2017heterogeneous}. It is a cross-project defect prediction where the source project and the target project have different feature spaces.
This method enables the source project to have a different set of features which was an impossible thing to do.
With this technique, it expanded the range of projects to be selected as a training set, which is very important because collecting buggy data is very hard.
Li et al. \cite{li2018cost} proposed cost-sensitive transfer kernel canonical correlation analysis (CTKCCA) to evaluate the nonlinear correlation relationship of the different features.
Li et al. \cite{li2019heterogeneous} proposed a two-staged ensemble learning (TSEL) approach for HDP, which contains an ensemble multi-kernel domain adaptation stage and ensemble data sampling stage. These stages handle separate nonlinear correlation of the features and the imbalance class of the labels.
Tong et al. \cite{tong2019kernel} proposed a kernel spectral embedding transfer ensemble (KSETE) which addresses the class imbalance problem, finds the latent common feature space by combining kernel spectral embedding.

\subsection{Just-in-Time Defect Prediction}
JIT-DP tackles another problem in the traditional defect prediction.
The actionability of traditional defect prediction is limited because usually, a predicted module is too big, making it very hard for the developers to act upon to fix the bug.
In JIT-DP, the granularity of the prediction is at the change-level, usually smaller than a whole source file, making it easier for the developers to act upon due to the smaller codebase.
Mockus et al. \cite{mockus2000identifying} proposed the first to identify changes concerning their specific reasons of causes: adding new features, correcting faults, and restructuring code for future changes.
Kim et al. \cite{kim2008classifying} is the first study that did a machine learning modeling for  a buggy change of a project.
Kamei et al. \cite{kamei2016studying} empirically evaluated the JIT-DP model in the context of a cross-project scenario.
They found that the models improve performance when selecting models that use other similar projects, using a larger pool of datasets, and using several projects for ensemble learning.  

These various defect prediction models use machine learning for prediction.
On the contrary, our method of defect prediction does not use any machine learning algorithms for prediction.
Even though we use autoencoder and kNN, it is for searching for similar commits.
The predictions are made by comparing the distance and cutting them with a threshold value.

\section{Automatic Program Repair}
Automatic program repair is a method that automatically generates a bug-fixing change, i.e. a patch, for a defective code.
We have added APR in the related studies because it could be related to our  patch suggestion method.

Weimer et al. \cite{weimer2009automatically} experimented on generating patches using genetic programming (GP) and repeated testing and generating until the test case passed. 
Kim et al. \cite{kim2013automatic} also used GP in generating patches, however, they pointed out that existing methods purely generated patches randomly. 
So they did not generate their patches in pure randomness, but they generated them by retrieving patches that are made by developers. 

The reason why patch suggestion is important even though there is a research field that generates patches is that APR has limited patch templates to generate.
Also, the generated patches must be applied to every test case which takes a very long time.
And because the generated patches are validated only on test cases, the patches are overfitted on the test cases.
The patch might pass every test case but it is not guaranteed to be a correct patch.
Our patch suggestion method has the potential to improve APR techniques by providing mutation operators and code ingredients.

