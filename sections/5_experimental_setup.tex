This chapter explains the settings of the experiment that is conducted in this study.

\section{Research Questions}
We considered two research questions to evaluate the defect prediction of {\simfinmo}.

\begin{itemize}
    \item RQ1: Was the defect prediction of {\simfinmo} potentially comparable to those of various machine learners?
    \item RQ2: How actionable are patches suggested by the {\simfinmo}?
    \item RQ3: What were the impacts of various {\simfinmo} cutoffs on prediction performance?
    \item RQ4: How effective is using the divided {\simfin} in terms of predictive performance?
\end{itemize}

We investigated the effectiveness of {\simfinmo} by comparing its prediction performance with the existing baseline.
Furthermore, we studied various aspects of {\simfinmo} using different cut-off values.

\section{Dataset}
We used 193 active, Java projects in the Apache Software Foundation (ASF) to construct {\simfin}.
We chose the projects that maintain active GitHub- or Jira-issued tracking systems.
A total of 110K BIC instances and 2.6M clean instances were collected for the training data to build the buggy and clean {\simfin} models.

Table \ref{tab:test_project} shows the details of the test project.
The test set used are also from ASF.
These test projects were selected by considering various buggy ratios and the different number of commits.
JIT-DP conducted at the change level remains very challenging to achieve high prediction performance.
One of the reasons is that the number of buggy commits is significantly smaller than that of clean commits.
These ratios are affected by the total number of commits in a project.
Thus, for our test data, we randomly chose six projects by considering various buggy ratios and the different number of commits.
As explained in the approach section, the two {\simfin} models are trained by the BIC instances or clean instances.

% test set list
\begin{table}[!htp]
\caption{The list of project used as a test set}
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{|l|l|l|l|}
\hline
Name & \# of Buggy & \# of Clean & Total \\ \hline
maven & 988 (9.2\%) & 10786 (90.8\%) & 11774 \\ \hline
ranger & 709 (12.2\%) & 5810 (87.8\%) & 6519 \\ \hline
sentry & 265 (10.8\%) & 2446 (89.2\%) & 2711 \\ \hline
sqoop & 91 (2.2\%) & 4204 (97.8\%) & 4295 \\ \hline
syncope & 1254 (4.6\%) & 26415 (95.4\%) & 27669 \\ \hline
tez & 1091 (16.5\%) & 6629 (83.5\%) & 7720 \\ \hline
median & 709 (9.2\%) & 6629 (90.8\%) &  7720\\ \hline
\end{tabular}%
% }
\newline
\label{tab:test_project}
\end{table}

\section{Baseline}
The baseline we use to compare the prediction performance is a typical JIT-DP metric reported in Kamei et al.~\cite{kamei2012large}.
The metric types are classified into five dimensions: diffusion, size, purpose, history,and experience. 

The \textbf{diffusion dimension} of a change shows how distributed a change is.
A distributed change can be measured by counting the different components of source files.
There are four features in this category: number of modified subsystems (NS), number of modified directories (ND), number of modified files (NF), and distribution of modified code(Entropy).
For our experiment, we considered the number of the subsystem as one because they are all Java projects.
For illustration, if a commit changed three files: java/src/clami/main.java, java/src/clami/utils.java, and the java/src/remi/input.java.
NS is one (java/src/), ND is two (clami/ and city/) and NF is three (main.java, input.java, and utils.java).
Entropy counts the distribution of modified lines in a source file.
\textbf{Size dimension} is the number of changed lines in a source file.
The three features of these categories are lines of code added (LA), lines of code deleted (LD), and lines of code in a file before the change (LT).
The \textbf{purpose dimension} has one feature that which is FIX.
FIX is a binary feature that labels if a commit is a BFC or not.
This is used as a feature because a BFC is more likely to introduce new bugs.
The \textbf{history dimension} is about the revision history of changes from the past to the present.
NDEV is the number of unique developers who have modified a source file.
AGE is the time between the current source and the most recent modification.
NUC is the number of unique changes in a commit.
For example, there are four source files in a commit that are A, B, C, and D. File A and B had been modified at $\alpha$ commit, file C had been modified at $\beta$ commit and file D had been modified at $\gamma$ commit.
In this case, NUC is three ($\alpha$, $\beta$, and $\gamma$). 
The \textbf{experience dimension} is the information of developers in the project.
Developers who frequently participate in the project is less likely to cause bugs because the developer understands the project well.
The experience dimension has two factors: Developer experience(EXP) and Recent developer experience(REXP).
EXP is the total number of commits the developer has created.
REXP is the number of commits that have been weighted according to the year the developer participated in.
A developer, for example, created one commit in 2017, three commits in 2018, and two commits in 2020.
REXP in 2020 is 3.25 (i.e., $\frac{2}{1} + \frac{3}{3}  + \frac{1}{4}$), and REXP in 2021 is 1.95 (i.e., $\frac{2}{2} + \frac{3}{4}  + \frac{1}{5}$).

With these metrics, an online change classification model is built as reported from Tan et al. \cite{tan2015online}.
Online change classification is proposed to address the limitation of cross-validation on change classification.
Firstly, cross-validation is not suitable in change classification because it will use future data for predicting bugs from the past.
This scenario does not match the real-world scenario because buggy change and a fixing change has a chronological relationship.
Secondly, there is a chance of mislabeling the changes. 
When we predict a buggy change in a certain period, the buggy change is labeled as buggy because there is a bug-fixing change in the future.
However, in the time the fixing didn't happen yet so it should be labeled as clean.

To address these issues, Tan et al. \cite{tan2015online} proposed an online change classification method.
They left a gap between the training data and the test data.
This gap should be an estimate of bug-fixing time so that there is enough time for the buggy changes in the trainset to be discovered.
Also, the method is to construct an online model, meaning that the training data is accumulated and updated as time goes by.
By doing this, the model is applied with multiple runs alleviating the sensitivity that can be dependent on a particular time.

In online change classification, the first two datasets cannot be used as a test set because when predicting the first dataset, the prediction time is before any bug-fixing is done and therefore there will be no buggy changes to predict from. The second data set cannot be used as well because at least one period has to skip to leave a gap.
For a fair comparison, the evaluation for {\simfinmo} does not predict the first two sets of data.

We have assessed six of the most used machine learning algorithms in defect prediction.
The algorithms we trained are BayesNet (BN), k-Nearest Neighbor (IBk), Logistic model tree (LMT), Naive Bayes (NB), and random forest (RF).

\subsection{Evaluation Metrics}
The evaluation metrics for comparing {\simfinmo} and baseline are precision, recall, F1 score and MCC.
We used a variety of evaluation metrics to assess a sound experiment and show various aspects of the predictors.
A Confusion matrix is needed to evaluate two models.
There are four metrics as shown in table \ref{tab:Confusion}.
True positive (TP) is when the actual label is true and the model predicts it as true.
False positive (FP) is when the actual label is false but the model predicts as true.
False negative (FN) is when the actual label is true but the model predicts as false.
True negative (TN) is when the actual label is false and the model predicts it as false.
Precision is the value of positive predictive that is the correct percentage of bugs among predicted bugs ($Precision = \frac{TP}{TP + FP}$) and recall is hit rate that is the percentage of predicted bugs among actual bugs ($Recall = \frac{TP}{TP + FN}$).
F1-score is the harmonic mean of the precision and recall ($F1-score = 2 \times \frac{Precision \times Recall}{Precision + Recall}$). 
MCC is the Matthews correlation coefficient that is a measure used in unbalanced labels ($MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$. 

\begin{table}[htbp]
\caption{Confusion matrix}
\centering
% \resizebox{\linewidth}{!}{%
\begin{tabular}{|c|c|c|}\hline

\backslashbox{Predicted}{Actual} & Buggy & Clean\\ \hline
Buggy & True Positive & False Positive \\ \hline
Clean & False Negative & True Negative \\ \hline
\end{tabular}%
% }
\newline
\label{tab:Confusion}
\end{table}